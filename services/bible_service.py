# ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ ê´€ë ¨ ë¡œì§
# ì„±ê²½ ë ˆí¼ë¨¼ìŠ¤ íŒŒì‹± ë¡œì§
def canonical_book_name(book: str) -> str:
    book_key = normalize_korean(book or '').replace(" ", "")
    if not book_key:
        return ''
    if book_key.lower() in BOOK_ABBREVIATIONS:
        return BOOK_ABBREVIATIONS[book_key.lower()]
    if book_key in BOOK_ABBREVIATIONS:
        return BOOK_ABBREVIATIONS[book_key]
    return BOOK_NAME_MAP.get(book_key, book_key)


def parse_reference_input(text: str):
    m = REFERENCE_INPUT_PATTERN.match(normalize_korean(text or ""))
    if not m:
        return None
    book_raw, chapter, verse, verse_end = m.groups()
    book = canonical_book_name(book_raw)
    if not book:
        return None
    return {
        "book": book,
        "chapter": int(chapter),
        "verse": int(verse),
        "verse_end": int(verse_end) if verse_end else None,
    }

# í•µì‹¬ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜
@app.route('/api/recommend-verses', methods=['POST'])
def recommend_verses():
    """ë ˆí¼ëŸ°ìŠ¤ ì§ì ‘ ë§¤ì¹­ â†’ ë¬¸êµ¬ ê²€ìƒ‰(greedy+semantic) ì¶”ì²œ."""
    data = request.get_json(silent=True) or {}
    query = (data.get('query') or data.get('keyword') or '').strip()
    page = 0
    try:
        page = max(0, int(data.get('page', 0)))
    except Exception:
        page = 0
    if not query:
        return jsonify({'error': 'ê²€ìƒ‰ì–´ê°€ í•„ìš”í•©ë‹ˆë‹¤'}), 400
    if not bible_collection:
        return recommend_verses_supabase(query, page)

    try:
        print(f"\nğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'")
        ensure_reference_index()
        ensure_verse_lookup_index()

        # 1) ë ˆí¼ëŸ°ìŠ¤ ì§ì ‘ ë§¤ì¹­ ë¨¼ì € ì‹œë„
        exact_hit = get_exact_verse_entry(query)
        if exact_hit:
            meta = exact_hit["metadata"] or {}
            ref_override = meta.get("_reference_override")
            if ref_override:
                reference = ref_override
            else:
                reference = build_reference_label(meta, exact_hit["text"])
            print(f"   ğŸ¯ ë ˆí¼ëŸ°ìŠ¤ ì§ì ‘ ë§¤ì¹­ ì„±ê³µ: {reference}")

            return jsonify({
                "verses": [
                    {
                        "reference": reference,
                        "text": exact_hit["text"],
                        "metadata": meta,
                        "score": 1.0,
                    }
                ]
            })
        else:
            print("   âš ï¸ ë ˆí¼ëŸ°ìŠ¤ ì§ì ‘ ë§¤ì¹­ ì—†ìŒ â†’ ì‹œë§¨í‹±/greedy ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰")

        # 2) í…Œë§ˆ í† í° ë§¤ì¹­ â†’ curated êµ¬ì ˆ ìš°ì„  ì£¼ì…
        query_text, curated_refs = build_contextual_query(query)
        curated_set = set()
        curated_items = []
        for ref in curated_refs:
            key = normalize_reference(ref)
            if not key or key in curated_set:
                continue
            curated_set.add(key)
            hit = get_or_create_curated_entry(key, ref)
            if hit:
                meta = hit.get("metadata") or {}
                doc = hit.get("text", "")
                reference = build_reference_label(meta, doc)
                pop = meta.get("popularity", 85)
                curated_items.append({
                    "reference": reference,
                    "text": doc,
                    "metadata": meta,
                    "score": 1.8,
                    "priority": "theme_top",
                    "popularity": pop,
                })
            else:
                print(f"     âš ï¸ ëŒ€í‘œ êµ¬ì ˆ ë¯¸ë°œê²¬: {ref}")
        if curated_items:
            print(f"   ğŸ¯ í…Œë§ˆ ëŒ€í‘œ êµ¬ì ˆ {len(curated_items)}ê°œ ì£¼ì…")

        # 3) ë¬¸êµ¬ ê²€ìƒ‰: greedy + semantic í˜¼í•©
        expanded_terms = greedy_terms(query)
        normalized_query = re.sub(r"\s+", "", normalize_korean(query or "").lower())
        print(f"   ğŸ” greedy í•µì‹¬ì–´: {expanded_terms if expanded_terms else 'ì—†ìŒ'}")
        query_embedding = embedding_model.encode(query_text).tolist()
        raw_results = bible_collection.query(
            query_embeddings=[query_embedding],
            n_results=200,
            include=["documents", "metadatas", "distances"],
        )

        docs = (raw_results.get("documents") or [[]])[0]
        metas = (raw_results.get("metadatas") or [[]])[0]
        dists = (raw_results.get("distances") or [[]])[0]

        scored = []
        for doc, meta, dist in zip(docs, metas, dists):
            if not doc:
                continue
            meta = meta or {}
            reference = meta.get("reference") or build_reference_label(meta, doc)
            if normalize_reference(reference) in curated_set:
                continue
            pop = meta.get("popularity", 0)
            semantic = 1 - dist if dist is not None else 0
            greedy_hits = greedy_match_count(expanded_terms, doc)
            greedy_bonus = min(0.18, greedy_hits * 0.06)
            coverage = greedy_hits / max(1, len(expanded_terms)) if expanded_terms else 0
            phrase_bonus = coverage * 0.1  # í•µì‹¬ì–´ ì»¤ë²„ë¦¬ì§€ ë³´ë„ˆìŠ¤
            if coverage >= 0.99:  # ëª¨ë“  í•µì‹¬ì–´ë¥¼ í¬í•¨í•˜ë©´ ì¶”ê°€ ê°€ì‚°
                phrase_bonus += 0.08
            if normalized_query and normalized_query in re.sub(r"\s+", "", normalize_korean(doc or "").lower()):
                phrase_bonus += 0.06  # ì „ì²´ ë¬¸êµ¬ê°€ ì—°ì†í•´ ë“¤ì–´ìˆìœ¼ë©´ ì¶”ê°€ ë³´ë„ˆìŠ¤
            phrase_bonus = min(0.24, phrase_bonus)
            final_score = semantic * 0.6 + (pop / 100.0) * 0.4 + phrase_bonus + greedy_bonus

            scored.append((final_score, reference, doc, meta))

        scored.sort(key=lambda x: x[0], reverse=True)
        all_candidates_full = curated_items + scored
        page_size = 3
        start_idx = page * page_size
        end_idx = start_idx + page_size
        # ìš”ì²­í•œ í˜ì´ì§€ê¹Œì§€ í•„ìš”í•œ ë§Œí¼ë§Œ ìŠ¬ë¼ì´ìŠ¤
        needed = end_idx
        all_candidates = all_candidates_full[:needed]
        page_slice = all_candidates[start_idx:end_idx]
        total_pages = (len(all_candidates_full) + page_size - 1) // page_size if all_candidates_full else 0

        verses = []
        for entry in page_slice:
            if isinstance(entry, tuple):
                score, reference, doc, meta = entry
            else:
                score = entry.get("score", entry.get("final_score", 1.0))
                reference = entry.get("reference")
                doc = entry.get("text")
                meta = entry.get("metadata", {})

            print(f"  ğŸ“Œ [{reference}] score={round(score,4)}")
            snippet = re.sub(r"\s+", " ", (doc or ""))[:120]
            print(f"     {snippet}...")
            verses.append(
                {
                    "reference": reference,
                    "text": doc,
                    "metadata": meta,
                    "score": score,
                }
            )

        has_more = end_idx < len(all_candidates_full)
        return jsonify({
            "verses": verses,
            "has_more": has_more,
            "total_pages": total_pages,
            "page": page,
        })
    
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"}), 500

def build_contextual_query(keyword: str):
    """í‚¤ì›Œë“œë¥¼ ìƒí™© ì„¤ëª… ë¬¸ì¥ìœ¼ë¡œ í™•ì¥í•˜ê³ , í…Œë§ˆë³„ ëŒ€í‘œ êµ¬ì ˆ ëª©ë¡ë„ í•¨ê»˜ ë°˜í™˜."""
    keyword = (keyword or '').strip()
    lowered = keyword.lower()
    matched_contexts = []
    curated_refs = []

    for rule in THEME_CONTEXT_RULES:
        tokens = rule["tokens"]
        if any(token in keyword for token in tokens) or any(token in lowered for token in tokens):
            matched_contexts.append(rule["description"])
            curated_refs.extend(rule.get("curated_references", []))

    if not matched_contexts:
        matched_contexts.append(DEFAULT_CONTEXT_DESCRIPTION)

    # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìˆœì„œ ìœ ì§€
    seen_ctx = set()
    unique_contexts = []
    for ctx in matched_contexts:
        if ctx not in seen_ctx:
            unique_contexts.append(ctx)
            seen_ctx.add(ctx)

    seen_refs = set()
    unique_refs = []
    for ref in curated_refs:
        ref = ref.strip()
        if ref and ref not in seen_refs:
            unique_refs.append(ref)
            seen_refs.add(ref)

    contextual_summary = ' / '.join(unique_contexts)
    expanded = (
        f"query: {keyword}. "
        f"ìƒí™©ê³¼ ê°ì •: {contextual_summary}. "
        "ì£¼ì œì™€ ë§ë‹¿ì€ ì„±ê²½ì˜ ì•½ì†, ìœ„ë¡œ, ê²©ë ¤, ë„ì „, í•˜ë‚˜ë‹˜ì˜ ì„±í’ˆê³¼ ê³„íšì„ ì°¾ëŠ”ë‹¤."
    )
    return expanded, unique_refs

# 3. ì¸ë±ìŠ¤ ìƒì„± ë° ì¿¼ë¦¬
REFERENCE_INDEX = {}

def ensure_reference_index():
    if not REFERENCE_INDEX_LOADED and bible_collection:
        build_reference_index()

def build_reference_index():
    """í…Œë§ˆ ëŒ€í‘œ êµ¬ì ˆì„ ë¹ ë¥´ê²Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë„ë¡ ë©”ëª¨ë¦¬ì— ì ì¬."""
    global REFERENCE_INDEX_LOADED
    if REFERENCE_INDEX_LOADED or not bible_collection or not ALL_CURATED_REFERENCES:
        REFERENCE_INDEX_LOADED = True
        return

    target_refs = {
        normalize_reference(ref): ref
        for ref in ALL_CURATED_REFERENCES
        if ref
    }
    target_refs.pop('', None)

    if not target_refs:
        REFERENCE_INDEX_LOADED = True
        return

    print("ğŸ”„ í…Œë§ˆ ëŒ€í‘œ êµ¬ì ˆ ì¸ë±ìŠ¤ ë¡œë”© ì¤‘...")
    try:
        data = bible_collection.get(include=["documents", "metadatas"])
    except Exception as e:
        print(f"âš ï¸ ëŒ€í‘œ êµ¬ì ˆ ì¸ë±ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {e}")
        return

    docs = data.get("documents") or []
    metas = data.get("metadatas") or []
    found = 0

    for doc, meta in zip(docs, metas):
        reference = build_reference_label(meta, doc)
        normalized = normalize_reference(reference)
        if normalized in target_refs and normalized not in REFERENCE_INDEX:
            REFERENCE_INDEX[normalized] = {
                "text": doc,
                "metadata": meta,
            }
            found += 1
            if found == len(target_refs):
                break

    REFERENCE_INDEX_LOADED = True
    print(f"âœ… ëŒ€í‘œ êµ¬ì ˆ ì¸ë±ìŠ¤ ì¤€ë¹„ ì™„ë£Œ: {len(REFERENCE_INDEX)}ê°œ ë§¤í•‘")


def _collect_all_curated_references():
    seen = set()
    refs = []
    for rule in THEME_CONTEXT_RULES:
        for ref in rule.get("curated_references", []):
            if not ref:
                continue
            cleaned = normalize_korean(ref.strip())
            if cleaned not in seen:
                seen.add(cleaned)
                refs.append(cleaned)
    return refs


ALL_CURATED_REFERENCES = _collect_all_curated_references()
REFERENCE_INDEX = {}
REFERENCE_INDEX_LOADED = False
VERSE_LOOKUP_INDEX = {}
VERSE_LOOKUP_INDEX_LOADED = False


def canonical_book_name(book: str) -> str:
    book_key = normalize_korean(book or '').replace(" ", "")
    if not book_key:
        return ''
    if book_key.lower() in BOOK_ABBREVIATIONS:
        return BOOK_ABBREVIATIONS[book_key.lower()]
    if book_key in BOOK_ABBREVIATIONS:
        return BOOK_ABBREVIATIONS[book_key]
    return BOOK_NAME_MAP.get(book_key, book_key)


def parse_reference_input(text: str):
    m = REFERENCE_INPUT_PATTERN.match(normalize_korean(text or ""))
    if not m:
        return None
    book_raw, chapter, verse, verse_end = m.groups()
    book = canonical_book_name(book_raw)
    if not book:
        return None
    return {
        "book": book,
        "chapter": int(chapter),
        "verse": int(verse),
        "verse_end": int(verse_end) if verse_end else None,
    }

def split_reference(reference: str):
    reference = normalize_korean(reference or '').strip()
    reference = reference.split('(')[0].strip()
    if not reference:
        return '', ''
    match = REFERENCE_SPLIT_PATTERN.match(reference)
    if match:
        book_raw = match.group(1).strip()
        remainder = match.group(2).strip()
    else:
        book_raw = reference
        remainder = ''
    book = canonical_book_name(book_raw)
    if remainder:
        remainder = remainder.strip()
        # ë²”ìœ„ê°€ ë¶™ì–´ ìˆìœ¼ë©´ ì‹œì‘ ì ˆë§Œ ì‚¬ìš©
        for sep in ['-', 'â€“', 'â€”', '~']:
            if sep in remainder:
                remainder = remainder.split(sep)[0].strip()
                break
    remainder = remainder.strip()
    return book, remainder


def normalize_reference(reference: str) -> str:
    """êµ¬ì ˆ í‘œì‹œ ë°©ì‹ì´ ì¡°ê¸ˆì”© ë‹¬ë¼ë„ ë¹„êµê°€ ê°€ëŠ¥í•˜ë„ë¡ ì •ê·œí™”."""
    book, remainder = split_reference(reference)
    if book and remainder:
        base = f"{book} {remainder}"
    elif book:
        base = book
    else:
        base = remainder
    return base.replace(" ", "")


def build_reference_label(metadata: dict, document: str) -> str:
    """ë©”íƒ€ë°ì´í„°ì™€ ë³¸ë¬¸ì—ì„œ ì±… ì´ë¦„ + ì¥:ì ˆì„ ì¡°í•©í•´ ì‚¬ëŒì´ ì½ì„ ë ˆí¼ëŸ°ìŠ¤ë¥¼ ë§Œë“ ë‹¤."""
    reference_field = metadata.get("reference") or ""
    source_field = metadata.get("source") or ""
    ref_book, ref_numbers = split_reference(reference_field)
    source_book = canonical_book_name(source_field)
    book = ref_book or source_book
    chapter_verse = extract_chapter_verse(document or "") if document else None

    if not chapter_verse and ref_numbers:
        chapter_verse = ref_numbers

    if book and chapter_verse:
        return f"{book} {chapter_verse}"
    if book:
        return book
    if chapter_verse:
        return chapter_verse
    return "ì•Œ ìˆ˜ ì—†ëŠ” êµ¬ì ˆ"


def build_reference_index():
    """í…Œë§ˆ ëŒ€í‘œ êµ¬ì ˆì„ ë¹ ë¥´ê²Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë„ë¡ ë©”ëª¨ë¦¬ì— ì ì¬."""
    global REFERENCE_INDEX_LOADED
    if REFERENCE_INDEX_LOADED or not bible_collection or not ALL_CURATED_REFERENCES:
        REFERENCE_INDEX_LOADED = True
        return

    target_refs = {
        normalize_reference(ref): ref
        for ref in ALL_CURATED_REFERENCES
        if ref
    }
    target_refs.pop('', None)

    if not target_refs:
        REFERENCE_INDEX_LOADED = True
        return

    print("ğŸ”„ í…Œë§ˆ ëŒ€í‘œ êµ¬ì ˆ ì¸ë±ìŠ¤ ë¡œë”© ì¤‘...")
    try:
        data = bible_collection.get(include=["documents", "metadatas"])
    except Exception as e:
        print(f"âš ï¸ ëŒ€í‘œ êµ¬ì ˆ ì¸ë±ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {e}")
        return

    docs = data.get("documents") or []
    metas = data.get("metadatas") or []
    found = 0

    for doc, meta in zip(docs, metas):
        reference = build_reference_label(meta, doc)
        normalized = normalize_reference(reference)
        if normalized in target_refs and normalized not in REFERENCE_INDEX:
            REFERENCE_INDEX[normalized] = {
                "text": doc,
                "metadata": meta,
            }
            found += 1
            if found == len(target_refs):
                break

    REFERENCE_INDEX_LOADED = True
    print(f"âœ… ëŒ€í‘œ êµ¬ì ˆ ì¸ë±ìŠ¤ ì¤€ë¹„ ì™„ë£Œ: {len(REFERENCE_INDEX)}ê°œ ë§¤í•‘")


def ensure_reference_index():
    if not REFERENCE_INDEX_LOADED and bible_collection:
        build_reference_index()


def build_verse_lookup_index():
    """(ì±…+ì¥:ì ˆ) â†’ ë¬¸ì„œ ì „ì²´ ì¸ë±ìŠ¤"""
    global VERSE_LOOKUP_INDEX_LOADED
    if VERSE_LOOKUP_INDEX_LOADED or not bible_collection:
        VERSE_LOOKUP_INDEX_LOADED = True
        return
    for doc, meta in iter_collection_documents(include=["documents", "metadatas"]):
        ref = build_reference_label(meta, doc)
        key = normalize_reference(ref)
        if key and key not in VERSE_LOOKUP_INDEX:
            VERSE_LOOKUP_INDEX[key] = {"text": doc, "metadata": meta}
    VERSE_LOOKUP_INDEX_LOADED = True


def ensure_verse_lookup_index():
    if not VERSE_LOOKUP_INDEX_LOADED and bible_collection:
        build_verse_lookup_index()


VERSE_LOOKUP_INDEX = {}
VERSE_LOOKUP_INDEX_LOADED = False

def iter_collection_documents(where=None, include=None, batch_size=2000):
    include = include or ["documents", "metadatas"]
    offset = 0
    while True:
        try:
            data = bible_collection.get(
                where=where,
                include=include,
                limit=batch_size,
                offset=offset,
            )
        except TypeError:
            data = bible_collection.get(where=where, include=include)
        docs = data.get("documents") or []
        metas = data.get("metadatas") or []
        if not docs:
            return
        for d, m in zip(docs, metas):
            yield d, (m or {})
        offset += len(docs)


def build_verse_lookup_index():
    global VERSE_LOOKUP_INDEX_LOADED
    if VERSE_LOOKUP_INDEX_LOADED or not bible_collection:
        VERSE_LOOKUP_INDEX_LOADED = True
        return

    for doc, meta in iter_collection_documents(include=["documents", "metadatas"]):
        ref = build_reference_label(meta, doc)
        key = normalize_reference(ref)
        if key and key not in VERSE_LOOKUP_INDEX:
            VERSE_LOOKUP_INDEX[key] = {"text": doc, "metadata": meta}

    VERSE_LOOKUP_INDEX_LOADED = True


def ensure_verse_lookup_index():
    if not VERSE_LOOKUP_INDEX_LOADED and bible_collection:
        build_verse_lookup_index()


def extract_exact_verse_text(book, chapter, verse, document):
    doc_norm = normalize_korean(document or "")
    abbrs = FULL_BOOK_TO_ABBREVIATIONS.get(book, [])
    for abbr in abbrs:
        start = re.search(
            rf'{re.escape(abbr)}\s*{chapter}\s*:\s*{verse}\s*',
            doc_norm,
        )
        if not start:
            continue
        nxt = re.search(
            r'\n?\s*[ê°€-í£]{1,5}\s*\d+\s*:\s*\d+\s*',
            doc_norm[start.end():],
        )
        end_idx = start.end() + (nxt.start() if nxt else len(doc_norm))
        body = doc_norm[start.end():end_idx].strip()
        return f"{abbr}{chapter}:{verse} {body}".strip()
    return None


def get_exact_verse_entry(ref_input: str):
    parsed = parse_reference_input(ref_input)
    if not parsed:
        return None

    book = parsed["book"]
    chapter = parsed["chapter"]
    verse = parsed["verse"]
    target_label = f"{book} {chapter}:{verse}"
    target_key = normalize_reference(target_label)

    ensure_verse_lookup_index()
    if target_key in VERSE_LOOKUP_INDEX:
        return VERSE_LOOKUP_INDEX[target_key]

    def doc_has_target(doc: str):
        doc_compact = re.sub(r"\s+", "", normalize_korean(doc or ""))
        markers = [
            f"{abbr}{chapter}:{verse}"
            for abbr in FULL_BOOK_TO_ABBREVIATIONS.get(book, [])
        ]
        markers += [re.sub(r"\s+", "", f"{book} {chapter}:{verse}")]
        return any(m in doc_compact for m in markers if m)

    for src in [book, KOREAN_TO_ENGLISH_BOOK.get(book)]:
        if not src:
            continue
        for doc, meta in iter_collection_documents(
            where={"source": src},
            include=["documents", "metadatas"],
        ):
            if normalize_reference(build_reference_label(meta, doc)) == target_key:
                return {"text": doc, "metadata": meta}
            if doc_has_target(doc):
                text = extract_exact_verse_text(book, chapter, verse, doc) or doc
                meta = dict(meta or {})
                meta["_reference_override"] = target_label
                return {"text": text, "metadata": meta}

    try:
        emb = embedding_model.encode(f"{target_label} ì„±ê²½ êµ¬ì ˆ").tolist()
        res = bible_collection.query(
            query_embeddings=[emb],
            n_results=200,
            include=["documents", "metadatas", "distances"],
        )
        docs = (res.get("documents") or [[]])[0]
        metas = (res.get("metadatas") or [[]])[0]
        for doc, meta in zip(docs, metas):
            if normalize_reference(build_reference_label(meta, doc)) == target_key:
                return {"text": doc, "metadata": meta}
            if doc_has_target(doc):
                text = extract_exact_verse_text(book, chapter, verse, doc) or doc
                meta = dict(meta or {})
                meta["_reference_override"] = target_label
                return {"text": text, "metadata": meta}
    except Exception:
        pass

    return None


def get_or_create_curated_entry(normalized_key: str, reference_label: str):
    if not normalized_key:
        return None
    cached = REFERENCE_INDEX.get(normalized_key)
    if cached:
        return cached
    hit = get_exact_verse_entry(reference_label)
    if hit:
        REFERENCE_INDEX[normalized_key] = hit
        return hit
    return None
