# app.py
from flask import Flask, render_template, request, jsonify
import chromadb
import uuid
from datetime import datetime
from apscheduler.schedulers.background import BackgroundScheduler
from sentence_transformers import SentenceTransformer
import os
import re
from dotenv import load_dotenv
from popular_verses import (
    get_popularity_score,
    extract_chapter_verse,
    normalize_korean,
    BOOK_NAME_MAP,
)  # â­ ì¶”ê°€

load_dotenv()

CHROMA_PATH = os.environ.get("CHROMA_PATH")
if not CHROMA_PATH:
    if os.path.exists("./vectordb_e5small"):
        CHROMA_PATH = "./vectordb_e5small"
    else:
        CHROMA_PATH = "./vectordb2"

EMBEDDING_MODEL_NAME = os.environ.get(
    "EMBEDDING_MODEL", "intfloat/multilingual-e5-small"
)

app = Flask(__name__)

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
print("ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
embedding_dim = embedding_model.get_sentence_embedding_dimension()
print(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {EMBEDDING_MODEL_NAME} ({embedding_dim}ì°¨ì›)")

# ChromaDB ì´ˆê¸°í™”
try:
    print(f"ğŸ“ ChromaDB ê²½ë¡œ: {CHROMA_PATH}")
    chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)
    bible_collection = chroma_client.get_collection(name="bible")
    print(f"âœ… ì»¬ë ‰ì…˜ ë¡œë“œ ì„±ê³µ: {bible_collection.name}")
    print(f"   ì´ êµ¬ì ˆ ìˆ˜: {bible_collection.count()}")
except Exception as e:
    print(f"âŒ ChromaDB ì—ëŸ¬: {e}")
    bible_collection = None

# ê²€ìƒ‰ ì£¼ì œë¥¼ ë¬¸ë§¥/ëŒ€í‘œ êµ¬ì ˆê³¼ í•¨ê»˜ í™•ì¥í•˜ê¸° ìœ„í•œ íŒíŠ¸ ì„¸íŠ¸
DEFAULT_CONTEXT_DESCRIPTION = (
    'ìœ„ë¡œì™€ ê²©ë ¤, í•˜ë‚˜ë‹˜ì˜ ì‹ ì‹¤í•˜ì‹¬, íšŒë³µê³¼ ì†Œë§, ë‘ë ¤ì›€ì„ ì´ê¸°ëŠ” ë¯¿ìŒ, ì‚¬ë‘ê³¼ ìš©ê¸°'
)

THEME_CONTEXT_RULES = [
    {
        "tokens": ['ì·¨ì—…', 'ì§„ë¡œ', 'ì§ì¥', 'ì»¤ë¦¬ì–´', 'íšŒì‚¬'],
        "description": 'ì·¨ì—…ê³¼ ì§„ë¡œ, ì¥ë˜ì˜ ê¸¸, í•˜ë‚˜ë‹˜ì˜ ê³µê¸‰ê³¼ ì¸ë„, ë‘ë ¤ì›€ ëŒ€ì‹  ë‹´ëŒ€í•¨',
        "curated_references": [
            "ì ì–¸ 16:3",
            "ì ì–¸ 3:5-6",
            "ì˜ˆë ˆë¯¸ì•¼ 29:11",
            "ì‹œí¸ 37:23",
            "ë¹Œë¦½ë³´ì„œ 4:13",
        ],
    },
    {
        "tokens": ['ì‹œí—˜', 'ê³µë¶€', 'í•™ì—…', 'ì…ì‹œ'],
        "description": 'ì§€í˜œì™€ ì¸ë‚´, ì„±ì‹¤í•˜ê²Œ ì¤€ë¹„í•˜ëŠ” ë§ˆìŒ, í•˜ë‚˜ë‹˜ê»˜ ë§¡ê¸°ëŠ” ë¯¿ìŒ',
        "curated_references": [
            "ì•¼ê³ ë³´ì„œ 1:5",
            "ê³ ë¦°ë„ì „ì„œ 10:13",
            "ë¹Œë¦½ë³´ì„œ 4:6",
            "ë¹Œë¦½ë³´ì„œ 4:13",
            "ì ì–¸ 2:6",
        ],
    },
    {
        "tokens": ['ìœ„ë¡œ', 'ìŠ¬í””', 'ëˆˆë¬¼', 'ìƒì‹¤', 'ì•„í””', 'ê³ í†µ'],
        "description": 'ìœ„ë¡œì™€ íšŒë³µ, í•¨ê»˜í•˜ì‹œëŠ” í•˜ë‚˜ë‹˜, ëˆˆë¬¼ì„ ë‹¦ì•„ì£¼ì‹œëŠ” ì‚¬ë‘',
        "curated_references": [
            "ì‹œí¸ 119:50",
            "ì´ì‚¬ì•¼ 41:10",
            "ì‹œí¸ 34:18",
            "ë§ˆíƒœë³µìŒ 11:28",
            "ì‹œí¸ 147:3",
        ],
    },
    {
        "tokens": ['ì†Œë§', 'í¬ë§', 'ë¯¸ë˜', 'ì¥ë˜'],
        "description": 'ì†Œë§ê³¼ ë¯¸ë˜ì— ëŒ€í•œ ì•½ì†, í•˜ë‚˜ë‹˜ì´ ì˜ˆë¹„í•˜ì‹  ê³„íšì„ ì‹ ë¢°í•¨',
        "curated_references": [
            "ì˜ˆë ˆë¯¸ì•¼ 29:11",
            "ê³ ë¦°ë„ì „ì„œ 13:13",
            "ë¡œë§ˆì„œ 15:13",
            "íˆë¸Œë¦¬ì„œ 11:1",
            "ì‹œí¸ 71:14",
        ],
    },
    {
        "tokens": ['ë‘ë ¤ì›€', 'ê±±ì •', 'ê·¼ì‹¬', 'ë¶ˆì•ˆ'],
        "description": 'ë‘ë ¤ì›€ì„ ì´ê¸°ëŠ” ë¯¿ìŒ, í‰ì•ˆ, ë‹´ëŒ€í•¨, ì—¼ë ¤ë¥¼ ë§¡ê¹€',
        "curated_references": [
            "ì´ì‚¬ì•¼ 41:10",
            "ë¹Œë¦½ë³´ì„œ 4:6-7",
            "ë§ˆíƒœë³µìŒ 6:34",
            "ì‹œí¸ 56:3",
            "ë””ëª¨ë°í›„ì„œ 1:7",
        ],
    },
    {
        "tokens": ['ê°ì‚¬', 'ê¸°ì¨', 'ì°¬ì–‘'],
        "description": 'ê°ì‚¬ì™€ ì°¬ì–‘, ê¸°ì¨ê³¼ ì¦ê±°ì›€, í•˜ë‚˜ë‹˜ì˜ ì„ í•˜ì‹¬',
        "curated_references": [
            "ì‹œí¸ 100:4",
            "ë°ì‚´ë¡œë‹ˆê°€ì „ì„œ 5:18",
            "ì‹œí¸ 16:11",
            "ë¹Œë¦½ë³´ì„œ 4:4",
            "ëŠí—¤ë¯¸ì•¼ 8:10",
        ],
    },
    {
        "tokens": ['ìš©ì„œ', 'ì£„ì±…ê°', 'íšŒê°œ'],
        "description": 'ìš©ì„œì™€ íšŒê°œ, ìƒˆ ë§ˆìŒ, ì€í˜œë¡œ ë‹¤ì‹œ ì‹œì‘í•¨',
        "curated_references": [
            "ìš”í•œì¼ì„œ 1:9",
            "ëˆ„ê°€ë³µìŒ 17:3-4",
            "ì—ë² ì†Œì„œ 4:32",
            "ì‹œí¸ 103:12",
            "ë¯¸ê°€ 7:19",
        ],
    },
    {
        "tokens": ['ì‚¬ë‘', 'ì—°ì• ', 'ê²°í˜¼', 'ë¶€ë¶€', 'ê°€ì •', 'ë¶€ëª¨', 'ìë…€', 'ê°€ì¡±'],
        "description": 'ì‚¬ë‘ê³¼ ì—°í•©, ê°€ì •ê³¼ ê´€ê³„ íšŒë³µ, ì„œë¡œë¥¼ ì„¸ì›Œ ì¤Œ',
        "curated_references": [
            "ê³ ë¦°ë„ì „ì„œ 13:4-7",
            "ìš”í•œì¼ì„œ 4:8",
            "ì—ë² ì†Œì„œ 5:25",
            "ì ì–¸ 17:17",
            "ê³¨ë¡œìƒˆì„œ 3:13",
        ],
    },
    {
        "tokens": ['ìš°ì •', 'ê³µë™ì²´', 'êµíšŒ', 'í˜•ì œ'],
        "description": 'ê³µë™ì²´ì™€ ìš°ì •, ì„œë¡œë¥¼ ê²©ë ¤í•˜ê³  ì„¸ì›Œ ì£¼ëŠ” ê´€ê³„',
        "curated_references": [
            "ìš”í•œë³µìŒ 15:13",
            "ì ì–¸ 17:17",
            "ì ì–¸ 27:17",
            "ìš”í•œë³µìŒ 17:21",
            "íˆë¸Œë¦¬ì„œ 10:24-25"
        ],
    },
    {
        "tokens": ['ì‚¬ëª…', 'í—Œì‹ ', 'ì„¬ê¹€', 'ìˆœì¢…'],
        "description": 'ì‚¬ëª…ê³¼ ìˆœì¢…, í—Œì‹ ê³¼ ì‚¬ë‘ìœ¼ë¡œ ì„¬ê¸°ëŠ” ì‚¶',
        "curated_references": [
            "ìš”í•œë³µìŒ 14:15",
            "ë¡œë§ˆì„œ 12:1",
            "ì‹ ëª…ê¸° 10:12",
            "ë§ˆíƒœë³µìŒ 16:24",
            "ê°ˆë¼ë””ì•„ì„œ 2:20"
        ],
    },
    {
        "tokens": ['ê±´ê°•', 'ì§ˆë³‘', 'ì¹˜ìœ ', 'íšŒë³µ'],
        "description": 'ì¹˜ìœ ì™€ íšŒë³µ, ê°•ê±´í•¨, ì•½í•œ ìë¥¼ ì„¸ìš°ì‹œëŠ” í•˜ë‚˜ë‹˜',
        "curated_references": [
            "ì•¼ê³ ë³´ì„œ 5:15",
            "ì¶œì• êµ½ê¸° 15:26",
            "ì‹œí¸ 103:3",
            "ë§ˆê°€ë³µìŒ 5:34"
        ],
    },
    {
        "tokens": ['ì¬ì •', 'ëˆ', 'í•„ìš”', 'ê¶í•', 'ê°€ë‚œ'],
        "description": 'í•„ìš”ë¥¼ ì±„ìš°ì‹œëŠ” í•˜ë‚˜ë‹˜, ê³µê¸‰ê³¼ ë§Œì¡±, ë‚˜ëˆ”ê³¼ ì‹ ë¢°',
        "curated_references": [
            "ë¹Œë¦½ë³´ì„œ 4:19",
            "ë§ˆíƒœë³µìŒ 6:33",
            "íˆë¸Œë¦¬ì„œ 13:5",
            "ì ì–¸ 30:8",
            "ë§ˆíƒœë³µìŒ 6:26",
        ],
    },
    {
        "tokens": ['ê°ˆë“±', 'ë¶„ë…¸', 'ì‹¸ì›€'],
        "description": 'í™”í•´ì™€ ìš©ì„œ, í‰í™”, ì‚¬ë‘ìœ¼ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•¨',
        "curated_references": [
            "ì•¼ê³ ë³´ì„œ 1:19-20",
            "ì ì–¸ 15:1",
            "ì—ë² ì†Œì„œ 4:26",
            "ë§ˆíƒœë³µìŒ 18:15",
            "ì ì–¸ 16:32"
        ],
    },
    {
        "tokens": ['í‰ì•ˆ', 'ì‰¼', 'ì•ˆì‹', 'ìƒ¬ë¡¬'],
        "description": 'í‰ì•ˆê³¼ ì•ˆì‹, í­í’ ê°€ìš´ë°ë„ ì§€í‚¤ì‹œëŠ” í•˜ë‚˜ë‹˜',
        "curated_references": [
            "ìš”í•œë³µìŒ 14:27",
            "ë§ˆíƒœë³µìŒ 11:28",
            "ì‹œí¸ 4:8",
            "ë¹Œë¦½ë³´ì„œ 4:7",
            "ìš”í•œë³µìŒ 16:33"
        ],
    },
]

REFERENCE_SPLIT_PATTERN = re.compile(r'^(.*?)(\d+:\d.*)$')
# ì˜ˆ: "ë¡¬15:13", "Rom 15:13", "1Th 5:18", "ì•½5:15" ë“±
DOC_REFERENCE_PATTERN = re.compile(r'([0-9]{0,1}\s*[ê°€-í£A-Za-z]{1,20})\s*(\d+:\d+)')
DOC_REFERENCE_PATTERN_JANG = re.compile(r'([0-9]{0,1}\s*[ê°€-í£A-Za-z]{1,20})\s*(\d+)\s*ì¥\s*(\d+)\s*ì ˆ')

# ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” "ë§ˆ 10:5", "ë§ˆ10ì¥5ì ˆ", "Matthew 10:5" ê°™ì€ ë ˆí¼ëŸ°ìŠ¤ íŒŒì„œ
REFERENCE_INPUT_PATTERN_RANGE = re.compile(
    r'^\s*([0-9]{0,1}\s*[ê°€-í£A-Za-z]{1,30})\s*([0-9]{1,3})\s*(?:[:ì¥]\s*([0-9]{1,3}))\s*(?:[-â€“â€”~]\s*([0-9]{1,3}))?\s*(?:ì ˆ)?\s*$'
)

BOOK_ABBREVIATIONS = {
    "ì°½": "ì°½ì„¸ê¸°",
    "ì¶œ": "ì¶œì• êµ½ê¸°",
    "ë ˆ": "ë ˆìœ„ê¸°",
    "ë¯¼": "ë¯¼ìˆ˜ê¸°",
    "ì‹ ": "ì‹ ëª…ê¸°",
    "ìˆ˜": "ì—¬í˜¸ìˆ˜ì•„",
    "ì‚¿": "ì‚¬ì‚¬ê¸°",
    "ë£»": "ë£»ê¸°",
    "ì‚¼ìƒ": "ì‚¬ë¬´ì—˜ìƒ",
    "ì‚¼í•˜": "ì‚¬ë¬´ì—˜í•˜",
    "ì™•ìƒ": "ì—´ì™•ê¸°ìƒ",
    "ì™•í•˜": "ì—´ì™•ê¸°í•˜",
    "ëŒ€ìƒ": "ì—­ëŒ€ìƒ",
    "ëŒ€í•˜": "ì—­ëŒ€í•˜",
    "ìŠ¤": "ì—ìŠ¤ë¼",
    "ëŠ": "ëŠí—¤ë¯¸ì•¼",
    "ì—": "ì—ìŠ¤ë”",
    "ìš¥": "ìš¥ê¸°",
    "ì‹œ": "ì‹œí¸",
    "ì ": "ì ì–¸",
    "ì „": "ì „ë„ì„œ",
    "ì•„": "ì•„ê°€",
    "ì‚¬": "ì´ì‚¬ì•¼",
    "ë ˜": "ì˜ˆë ˆë¯¸ì•¼",
    "ì• ": "ì˜ˆë ˆë¯¸ì•¼ì• ê°€",
    "ê²”": "ì—ìŠ¤ê²”",
    "ë‹¨": "ë‹¤ë‹ˆì—˜",
    "í˜¸": "í˜¸ì„¸ì•„",
    "ìšœ": "ìš”ì—˜",
    "ì•”": "ì•„ëª¨ìŠ¤",
    "ì˜µ": "ì˜¤ë°”ëŒœ",
    "ìš˜": "ìš”ë‚˜",
    "ë¯¸": "ë¯¸ê°€",
    "ë‚˜": "ë‚˜í›”",
    "í•©": "í•˜ë°•êµ­",
    "ìŠµ": "ìŠ¤ë°”ëƒ",
    "í•™": "í•™ê°œ",
    "ìŠ¥": "ìŠ¤ê°€ë´",
    "ë§": "ë§ë¼ê¸°",
    "ë§ˆ": "ë§ˆíƒœë³µìŒ",
    "ë§‰": "ë§ˆê°€ë³µìŒ",
    "ëˆ…": "ëˆ„ê°€ë³µìŒ",
    "ìš”": "ìš”í•œë³µìŒ",
    "í–‰": "ì‚¬ë„í–‰ì „",
    "ë¡¬": "ë¡œë§ˆì„œ",
    "ê³ ì „": "ê³ ë¦°ë„ì „ì„œ",
    "ê³ í›„": "ê³ ë¦°ë„í›„ì„œ",
    "ê°ˆ": "ê°ˆë¼ë””ì•„ì„œ",
    "ì—¡": "ì—ë² ì†Œì„œ",
    "ë¹Œ": "ë¹Œë¦½ë³´ì„œ",
    "ê³¨": "ê³¨ë¡œìƒˆì„œ",
    "ì‚´ì „": "ë°ì‚´ë¡œë‹ˆê°€ì „ì„œ",
    "ì‚´í›„": "ë°ì‚´ë¡œë‹ˆê°€í›„ì„œ",
    "ë”¤ì „": "ë””ëª¨ë°ì „ì„œ",
    "ë”¤í›„": "ë””ëª¨ë°í›„ì„œ",
    "ë”›": "ë””ë„ì„œ",
    "ëª¬": "ë¹Œë ˆëª¬ì„œ",
    "íˆ": "íˆë¸Œë¦¬ì„œ",
    "ì•½": "ì•¼ê³ ë³´ì„œ",
    "ë²§ì „": "ë² ë“œë¡œì „ì„œ",
    "ë²§í›„": "ë² ë“œë¡œí›„ì„œ",
    "ìš”ì¼": "ìš”í•œì¼ì„œ",
    "ìš”ì´": "ìš”í•œì´ì„œ",
    "ìš”ì‚¼": "ìš”í•œì‚¼ì„œ",
    "ìœ ": "ìœ ë‹¤ì„œ",
    "ê³„": "ìš”í•œê³„ì‹œë¡",
    # English abbreviations (lowercase keys)
    "mt": "ë§ˆíƒœë³µìŒ",
    "mat": "ë§ˆíƒœë³µìŒ",
    "matt": "ë§ˆíƒœë³µìŒ",
    "mk": "ë§ˆê°€ë³µìŒ",
    "mrk": "ë§ˆê°€ë³µìŒ",
    "lk": "ëˆ„ê°€ë³µìŒ",
    "luk": "ëˆ„ê°€ë³µìŒ",
    "jn": "ìš”í•œë³µìŒ",
    "jhn": "ìš”í•œë³µìŒ",
    "ps": "ì‹œí¸",
    "psa": "ì‹œí¸",
    "prov": "ì ì–¸",
    "prv": "ì ì–¸",
    "rom": "ë¡œë§ˆì„œ",
    "1th": "ë°ì‚´ë¡œë‹ˆê°€ì „ì„œ",
    "1thess": "ë°ì‚´ë¡œë‹ˆê°€ì „ì„œ",
    "2th": "ë°ì‚´ë¡œë‹ˆê°€í›„ì„œ",
    "2thess": "ë°ì‚´ë¡œë‹ˆê°€í›„ì„œ",
    "eph": "ì—ë² ì†Œì„œ",
    "phil": "ë¹Œë¦½ë³´ì„œ",
    "jas": "ì•¼ê³ ë³´ì„œ",
}

KOREAN_TO_ENGLISH_BOOK = {v: k for k, v in BOOK_NAME_MAP.items()}
FULL_BOOK_TO_ABBREVIATIONS = {}
for _abbr, _full in BOOK_ABBREVIATIONS.items():
    # ë¬¸ì„œ í”„ë¦¬í”½ìŠ¤ëŠ” ëŒ€ê°œ í•œê¸€ ì•½ì–´(ì˜ˆ: ë§ˆ10:5) í˜•íƒœ
    if re.fullmatch(r"[ê°€-í£0-9]+", _abbr):
        FULL_BOOK_TO_ABBREVIATIONS.setdefault(_full, []).append(_abbr)


def _collect_all_curated_references():
    seen = set()
    refs = []
    for rule in THEME_CONTEXT_RULES:
        for ref in rule.get("curated_references", []):
            if not ref:
                continue
            cleaned = normalize_korean(ref.strip())
            if cleaned not in seen:
                seen.add(cleaned)
                refs.append(cleaned)
    return refs


ALL_CURATED_REFERENCES = _collect_all_curated_references()
REFERENCE_INDEX = {}
REFERENCE_INDEX_LOADED = False
VERSE_LOOKUP_INDEX = {}
VERSE_LOOKUP_INDEX_LOADED = False


def iter_collection_documents(where=None, include=None, batch_size=2000):
    """
    Chroma collection.get() ê²°ê³¼ê°€ ê¸°ë³¸ì ìœ¼ë¡œ limitì´ ê±¸ë ¤ ìˆëŠ” í™˜ê²½ì„ ëŒ€ë¹„í•´,
    offset/limit ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ìˆœíšŒí•œë‹¤.
    """
    if not bible_collection:
        return
    include = include or ["documents", "metadatas"]

    total = None
    try:
        total = bible_collection.count()
    except Exception:
        total = None

    offset = 0
    while True:
        try:
            data = bible_collection.get(
                where=where,
                include=include,
                limit=batch_size,
                offset=offset,
            )
        except TypeError:
            # offset/limit ë¯¸ì§€ì› ë²„ì „ì´ë©´ ë‹¨ê±´ í˜¸ì¶œë¡œ í´ë°±
            data = bible_collection.get(where=where, include=include)
            docs = data.get("documents") or []
            metas = data.get("metadatas") or []
            for doc, meta in zip(docs, metas):
                yield doc, (meta or {})
            return

        docs = data.get("documents") or []
        metas = data.get("metadatas") or []
        if not docs:
            return

        for doc, meta in zip(docs, metas):
            yield doc, (meta or {})

        offset += len(docs)
        if total is not None and offset >= total:
            return


def canonical_book_name(book: str) -> str:
    book = normalize_korean(book or '').strip()
    if not book:
        return ''
    book_key = normalize_korean(book).replace(" ", "")
    book_key_lower = book_key.lower()
    if book_key_lower in BOOK_ABBREVIATIONS:
        return BOOK_ABBREVIATIONS[book_key_lower]
    if book_key in BOOK_ABBREVIATIONS:
        return BOOK_ABBREVIATIONS[book_key]
    if book in BOOK_ABBREVIATIONS:
        return BOOK_ABBREVIATIONS[book]
    return BOOK_NAME_MAP.get(book, book)


def split_reference(reference: str):
    reference = normalize_korean(reference or '').strip()
    reference = reference.split('(')[0].strip()
    if not reference:
        return '', ''
    match = REFERENCE_SPLIT_PATTERN.match(reference)
    if match:
        book_raw = match.group(1).strip()
        remainder = match.group(2).strip()
    else:
        book_raw = reference
        remainder = ''
    book = canonical_book_name(book_raw)
    if remainder:
        remainder = remainder.strip()
        # ë²”ìœ„ê°€ ë¶™ì–´ ìˆìœ¼ë©´ ì‹œì‘ ì ˆë§Œ ì‚¬ìš©
        for sep in ['-', 'â€“', 'â€”', '~']:
            if sep in remainder:
                remainder = remainder.split(sep)[0].strip()
                break
    remainder = remainder.strip()
    return book, remainder


def normalize_reference(reference: str) -> str:
    """êµ¬ì ˆ í‘œì‹œ ë°©ì‹ì´ ì¡°ê¸ˆì”© ë‹¬ë¼ë„ ë¹„êµê°€ ê°€ëŠ¥í•˜ë„ë¡ ì •ê·œí™”."""
    book, remainder = split_reference(reference)
    if book and remainder:
        base = f"{book} {remainder}"
    elif book:
        base = book
    else:
        base = remainder
    return base.replace(" ", "")


def parse_reference_input(text: str):
    """
    ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë ˆí¼ëŸ°ìŠ¤(ì˜ˆ: 'ë§ˆ 10:5', 'ë§ˆ10ì¥5ì ˆ', 'Matthew 10:5')ë¥¼ íŒŒì‹±í•œë‹¤.
    ë°˜í™˜: {"book": str, "chapter": int, "verse": int, "verse_end": Optional[int]} ë˜ëŠ” None
    """
    normalized = normalize_korean(text or "").strip()
    if not normalized:
        return None
    match = REFERENCE_INPUT_PATTERN_RANGE.match(normalized)
    if not match:
        return None
    book_raw, chapter_str, verse_str, verse_end_str = match.groups()
    book = canonical_book_name(book_raw)
    if not book:
        return None
    chapter = int(chapter_str)
    verse = int(verse_str)
    verse_end = int(verse_end_str) if verse_end_str else None
    return {"book": book, "chapter": chapter, "verse": verse, "verse_end": verse_end}


def _candidate_source_values(book: str):
    book = canonical_book_name(book)
    candidates = []
    if book:
        candidates.append(book)
        english = KOREAN_TO_ENGLISH_BOOK.get(book)
        if english:
            candidates.append(english)
    # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìˆœì„œ ìœ ì§€
    seen = set()
    unique = []
    for item in candidates:
        if item and item not in seen:
            unique.append(item)
            seen.add(item)
    return unique


def get_exact_verse_entry(reference_input: str):
    """ì…ë ¥ ë ˆí¼ëŸ°ìŠ¤ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, í•´ë‹¹ êµ¬ì ˆ(ë˜ëŠ” í•´ë‹¹ êµ¬ì ˆì´ í¬í•¨ëœ ë¬¸ì„œ)ì„ ì •í™•íˆ ì°¾ì•„ ë°˜í™˜."""
    if not bible_collection:
        return None
    parsed = parse_reference_input(reference_input)
    if not parsed:
        return None

    book = parsed["book"]
    chapter = parsed["chapter"]
    verse = parsed["verse"]
    verse_end = parsed.get("verse_end")
    target_label = f"{book} {chapter}:{verse}"
    target_key = normalize_reference(target_label)
    target_range_key = (
        normalize_reference(f"{book} {chapter}:{verse}-{verse_end}") if verse_end else None
    )

    ensure_verse_lookup_index()
    if target_key in VERSE_LOOKUP_INDEX:
        return VERSE_LOOKUP_INDEX[target_key]
    if target_range_key and target_range_key in VERSE_LOOKUP_INDEX:
        return VERSE_LOOKUP_INDEX[target_range_key]

    def _doc_contains_target(doc: str) -> bool:
        doc_norm = normalize_korean(doc or "")
        doc_compact = re.sub(r"\s+", "", doc_norm)
        # '10:5'ë§Œìœ¼ë¡œëŠ” ì±…ì„ êµ¬ë¶„í•  ìˆ˜ ì—†ì–´ ì˜¤íƒì´ ìƒê¸°ë¯€ë¡œ, ì±…ê¹Œì§€ í¬í•¨ëœ ë§ˆì»¤ë§Œ ì‚¬ìš©í•œë‹¤.
        markers = []
        abbrs = FULL_BOOK_TO_ABBREVIATIONS.get(book, [])
        for abbr in abbrs:
            markers.append(f"{abbr}{chapter}:{verse}")
            markers.append(f"{abbr}{chapter}ì¥{verse}ì ˆ")
        markers.append(re.sub(r"\s+", "", f"{book} {chapter}:{verse}"))
        return any(m and m in doc_compact for m in markers)

    def _match_in_docs(docs, metas):
        for doc, meta in zip(docs or [], metas or []):
            label = build_reference_label(meta or {}, doc or "")
            if normalize_reference(label) == target_key:
                return {"text": doc, "metadata": meta or {}}
        return None

    # 1) source(book) í•„í„°ë¡œ ìš°ì„  íƒìƒ‰
    for source_value in _candidate_source_values(book):
        try:
            for doc, meta in iter_collection_documents(
                where={"source": source_value},
                include=["documents", "metadatas"],
                batch_size=2000,
            ):
                label = build_reference_label(meta or {}, doc or "")
                if normalize_reference(label) == target_key:
                    return {"text": doc, "metadata": meta or {}}
                if _doc_contains_target(doc):
                    extracted = extract_exact_verse_text(book, chapter, verse, doc) or doc
                    meta = dict(meta or {})
                    meta["_reference_override"] = target_label
                    return {"text": extracted, "metadata": meta}
        except Exception:
            continue

    # 2) ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ í›„ (ì •í™• ë ˆì´ë¸” ë§¤ì¹­ â†’ ë³¸ë¬¸ ë§ˆì»¤ ë§¤ì¹­) ìˆœìœ¼ë¡œ í´ë°±
    try:
        embedding = embedding_model.encode(f"{target_label} ì„±ê²½ êµ¬ì ˆ").tolist()
        results = bible_collection.query(
            query_embeddings=[embedding],
            n_results=200,
            include=["documents", "metadatas", "distances"],
        )
        docs = results.get("documents", [[]])[0]
        metas = results.get("metadatas", [[]])[0]
        dists = results.get("distances", [[]])[0] or []
        hit = _match_in_docs(docs, metas)
        if hit:
            return hit
        # ë ˆì´ë¸”ì´ ì •í™•íˆ ì—†ë”ë¼ë„, chunk ë‚´ë¶€ì— 'ë§ˆ10:5' ê°™ì€ ë§ˆì»¤ê°€ ìˆìœ¼ë©´ ê·¸ ë¬¸ì„œë¥¼ ë°˜í™˜
        best = None
        best_dist = None
        for doc, meta, dist in zip(docs, metas, dists):
            if _doc_contains_target(doc):
                if best is None or (isinstance(dist, (int, float)) and dist < best_dist):
                    extracted = extract_exact_verse_text(book, chapter, verse, doc) or doc
                    meta = dict(meta or {})
                    meta["_reference_override"] = target_label
                    best = {"text": extracted, "metadata": meta}
                    best_dist = dist
        if best:
            return best
    except Exception as exc:
        print(f"âš ï¸ ë ˆí¼ëŸ°ìŠ¤ ì§ì ‘ ê²€ìƒ‰ ì‹¤íŒ¨ ({reference_input}): {exc}")

    return None


def contains_phrase(query: str, document: str) -> bool:
    query = normalize_korean(query or "").strip()
    document = normalize_korean(document or "")
    if not query or len(query) < 2:
        return False
    query_compact = re.sub(r"\s+", "", query)
    doc_compact = re.sub(r"\s+", "", document)
    return query_compact in doc_compact


KOREAN_STOPWORDS = {
    "ìê¸°", "ë‚´", "ë‚˜", "ë„ˆ", "ìš°ë¦¬", "ë„ˆí¬", "ê·¸", "ê·¸ì˜", "ê·¸ë…€", "ê·¸ë“¤ì˜",
    "ì´", "ì €", "ê²ƒ", "ìˆ˜", "ë•Œ", "ë§", "ì¼", "ë“±",
    "ì„", "ë¥¼", "ì´", "ê°€", "ì€", "ëŠ”", "ì—", "ì˜", "ê³¼", "ì™€", "ë¡œ", "ìœ¼ë¡œ",
}


def greedy_terms(query: str):
    """
    êµ¬ì ˆ/ë¬¸êµ¬ ê²€ìƒ‰ì„ ìœ„í•´ ì§ˆì˜ì—ì„œ í•µì‹¬ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•œë‹¤(ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±).
    ì˜ˆ: 'ìê¸° ì‹­ìê°€ë¥¼' -> ['ì‹­ìê°€']
    """
    normalized = normalize_korean(query or "").strip().lower()
    if not normalized:
        return []
    terms = []
    for token in re.findall(r"[ê°€-í£]{2,}|[a-z]{2,}", normalized):
        token = token.strip()
        if token in KOREAN_STOPWORDS:
            continue
        if len(token) < 2:
            continue
        terms.append(token)
    seen = set()
    unique = []
    for t in terms:
        if t not in seen:
            unique.append(t)
            seen.add(t)
    return unique[:6]


def greedy_match_count(terms, document: str) -> int:
    if not terms or not document:
        return 0
    doc_norm = normalize_korean(document).lower()
    doc_compact = re.sub(r"\s+", "", doc_norm)
    count = 0
    for term in terms:
        term_compact = re.sub(r"\s+", "", term)
        if term_compact and term_compact in doc_compact:
            count += 1
    return count


def extract_exact_verse_text(book: str, chapter: int, verse: int, document: str) -> str | None:
    """
    chunk ë¬¸ì„œì— ì—¬ëŸ¬ ì ˆì´ í¬í•¨ëœ ê²½ìš°(ì˜ˆ: ë§ˆ10:4 ... \\në§ˆ10:5 ...),
    ìš”ì²­í•œ íŠ¹ì • ì ˆë§Œ ë³¸ë¬¸ìœ¼ë¡œ ì˜ë¼ë‚¸ë‹¤.
    """
    if not document:
        return None
    doc_norm = normalize_korean(document)
    abbrs = FULL_BOOK_TO_ABBREVIATIONS.get(book, [])
    if not abbrs:
        return None

    best = None
    best_pos = None
    for abbr in abbrs:
        # ì‹œì‘ ë§ˆì»¤: 'ë§ˆ10:5' ë˜ëŠ” 'ë§ˆ 10:5'
        start_pattern = re.compile(
            rf'({re.escape(abbr)})\s*{chapter}\s*:\s*{verse}\s*',
            re.MULTILINE,
        )
        start_match = start_pattern.search(doc_norm)
        if not start_match:
            continue

        start_idx = start_match.end()
        # ë‹¤ìŒ ì ˆ ë§ˆì»¤(ëŒ€ê°œ ê°™ì€ ì±… ì•½ì–´ë¡œ ì‹œì‘í•˜ì§€ë§Œ, ì•ˆì „í•˜ê²Œ í•œê¸€ì•½ì–´+ì¥:ì ˆ íŒ¨í„´ìœ¼ë¡œ ì¢…ë£Œ)
        next_marker = re.compile(r'\n?\s*[ê°€-í£]{1,5}\s*\d+\s*:\s*\d+\s*', re.MULTILINE)
        next_match = next_marker.search(doc_norm, start_idx)
        end_idx = next_match.start() if next_match else len(doc_norm)
        body = doc_norm[start_idx:end_idx].strip()
        candidate = f"{abbr}{chapter}:{verse} {body}".strip()

        if best is None or (start_match.start() < best_pos):
            best = candidate
            best_pos = start_match.start()

    return best


def build_reference_label(metadata: dict, document: str) -> str:
    """ë©”íƒ€ë°ì´í„°ì™€ ë³¸ë¬¸ì—ì„œ ì±… ì´ë¦„ + ì¥:ì ˆì„ ì¡°í•©í•´ ì‚¬ëŒì´ ì½ì„ ë ˆí¼ëŸ°ìŠ¤ë¥¼ ë§Œë“ ë‹¤."""
    reference_field = metadata.get("reference") or ""
    source_field = metadata.get("source") or ""
    ref_book, ref_numbers = split_reference(reference_field)
    source_book = canonical_book_name(source_field)
    book = ref_book or source_book
    chapter_verse = extract_chapter_verse(document or "") if document else None

    if not chapter_verse and ref_numbers:
        chapter_verse = ref_numbers

    if (not book or not chapter_verse) and document:
        normalized_doc = normalize_korean(document or "")
        doc_match = DOC_REFERENCE_PATTERN.search(normalized_doc[:80])
        if doc_match:
            doc_book_raw, doc_chapter = doc_match.groups()
            doc_book = canonical_book_name(doc_book_raw)
            if not book:
                book = doc_book
            if not chapter_verse:
                chapter_verse = doc_chapter
        elif (match := DOC_REFERENCE_PATTERN_JANG.search(normalized_doc[:80])):
            doc_book_raw, chapter, verse = match.groups()
            doc_book = canonical_book_name(doc_book_raw)
            if not book:
                book = doc_book
            if not chapter_verse:
                chapter_verse = f"{chapter}:{verse}"
        if not chapter_verse:
            detected = extract_chapter_verse(document or "")
            if detected:
                chapter_verse = detected

    if book and chapter_verse:
        return f"{book} {chapter_verse}"
    if book:
        return book
    if chapter_verse:
        return chapter_verse
    return "ì•Œ ìˆ˜ ì—†ëŠ” êµ¬ì ˆ"


def build_reference_index():
    """í…Œë§ˆ ëŒ€í‘œ êµ¬ì ˆì„ ë¹ ë¥´ê²Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë„ë¡ ë©”ëª¨ë¦¬ì— ì ì¬."""
    global REFERENCE_INDEX_LOADED, VERSE_LOOKUP_INDEX_LOADED
    if REFERENCE_INDEX_LOADED or not bible_collection or not ALL_CURATED_REFERENCES:
        REFERENCE_INDEX_LOADED = True
        return

    target_refs = {
        normalize_reference(ref): ref
        for ref in ALL_CURATED_REFERENCES
        if ref
    }
    target_refs.pop('', None)

    if not target_refs:
        REFERENCE_INDEX_LOADED = True
        return

    print("ğŸ”„ í…Œë§ˆ ëŒ€í‘œ êµ¬ì ˆ ì¸ë±ìŠ¤ ë¡œë”© ì¤‘...")
    found = 0

    try:
        iterator = iter_collection_documents(include=["documents", "metadatas"])
        for doc, meta in iterator:
            reference = build_reference_label(meta, doc)
            normalized = normalize_reference(reference)
            if normalized and normalized not in VERSE_LOOKUP_INDEX:
                VERSE_LOOKUP_INDEX[normalized] = {
                    "text": doc,
                    "metadata": meta,
                }
            if normalized in target_refs and normalized not in REFERENCE_INDEX:
                REFERENCE_INDEX[normalized] = {
                    "text": doc,
                    "metadata": meta,
                }
                found += 1
    except Exception as e:
        print(f"âš ï¸ ëŒ€í‘œ êµ¬ì ˆ ì¸ë±ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {e}")
        return

    missing_keys = [key for key in target_refs if key not in REFERENCE_INDEX]
    if missing_keys:
        print(f"   ğŸ” ì¸ë±ìŠ¤ ë¯¸í¬í•¨ ëŒ€í‘œ êµ¬ì ˆ {len(missing_keys)}ê°œ ì¶”ê°€ íƒìƒ‰")
        for key in missing_keys:
            original_ref = target_refs[key]
            entry = lookup_reference_with_query(original_ref)
            if not entry:
                entry = lookup_reference_by_book(original_ref)
            if entry:
                REFERENCE_INDEX[key] = entry
            else:
                print(f"     âš ï¸ ì´ˆê¸° ë¡œë”©ì—ì„œ ëŒ€í‘œ êµ¬ì ˆ ë¯¸ë°œê²¬: {original_ref}")

    REFERENCE_INDEX_LOADED = True
    VERSE_LOOKUP_INDEX_LOADED = True
    print(f"âœ… ëŒ€í‘œ êµ¬ì ˆ ì¸ë±ìŠ¤ ì¤€ë¹„ ì™„ë£Œ: {len(REFERENCE_INDEX)}ê°œ ë§¤í•‘")


def ensure_reference_index():
    if not REFERENCE_INDEX_LOADED and bible_collection:
        build_reference_index()


def build_verse_lookup_index():
    """ë ˆí¼ëŸ°ìŠ¤(ì±…+ì¥:ì ˆ) â†’ ë¬¸ì„œ/ë©”íƒ€ë°ì´í„° ì „ì²´ ì¸ë±ìŠ¤(ì§ì ‘ êµ¬ì ˆ ê²€ìƒ‰ìš©)."""
    global VERSE_LOOKUP_INDEX_LOADED
    if VERSE_LOOKUP_INDEX_LOADED or not bible_collection:
        VERSE_LOOKUP_INDEX_LOADED = True
        return
    print("ğŸ”„ ë ˆí¼ëŸ°ìŠ¤ ì „ì²´ ì¸ë±ìŠ¤ ë¡œë”© ì¤‘...")
    try:
        for doc, meta in iter_collection_documents(include=["documents", "metadatas"]):
            reference = build_reference_label(meta or {}, doc or "")
            key = normalize_reference(reference)
            if key and key not in VERSE_LOOKUP_INDEX:
                VERSE_LOOKUP_INDEX[key] = {"text": doc, "metadata": meta or {}}
    except Exception as e:
        print(f"âš ï¸ ë ˆí¼ëŸ°ìŠ¤ ì „ì²´ ì¸ë±ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {e}")
        return
    VERSE_LOOKUP_INDEX_LOADED = True
    print(f"âœ… ë ˆí¼ëŸ°ìŠ¤ ì „ì²´ ì¸ë±ìŠ¤ ì¤€ë¹„ ì™„ë£Œ: {len(VERSE_LOOKUP_INDEX)}ê°œ")


def ensure_verse_lookup_index():
    if not VERSE_LOOKUP_INDEX_LOADED and bible_collection:
        build_verse_lookup_index()


def lookup_reference_with_query(reference_label: str):
    """Missing curated êµ¬ì ˆì„ ì§ì ‘ ê²€ìƒ‰í•´ì„œ ì°¾ì•„ì˜¨ë‹¤."""
    if not bible_collection or not reference_label:
        return None
    target_book, _ = split_reference(reference_label)
    try:
        query_text = f"{reference_label} ì„±ê²½ êµ¬ì ˆì˜ ë³¸ë¬¸"
        embedding = embedding_model.encode(query_text).tolist()
        results = bible_collection.query(
            query_embeddings=[embedding],
            n_results=5,
            include=["documents", "metadatas"],
        )
        docs = results.get("documents", [[]])[0]
        metas = results.get("metadatas", [[]])[0]
        for doc, meta in zip(docs, metas):
            label = build_reference_label(meta, doc)
            if normalize_reference(label) == normalize_reference(reference_label):
                return {"text": doc, "metadata": meta}
        if target_book:
            for doc, meta in zip(docs, metas):
                candidate_label = build_reference_label(meta, doc)
                candidate_book, _ = split_reference(candidate_label)
                if candidate_book == target_book:
                    return {"text": doc, "metadata": meta}
    except Exception as exc:
        print(f"âš ï¸ ëŒ€í‘œ êµ¬ì ˆ ì§ì ‘ ì¡°íšŒ ì‹¤íŒ¨ ({reference_label}): {exc}")
    return None


def lookup_reference_by_book(reference_label: str):
    """ê°™ì€ ì±…ì— ì†í•œ metadataë¥¼ í›‘ì–´ í•´ë‹¹ ì¥/ì ˆì„ ì°¾ëŠ”ë‹¤."""
    if not bible_collection or not reference_label:
        return None
    book, _ = split_reference(reference_label)
    if not book:
        return None

    try:
        for doc, meta in iter_collection_documents(
            where={"source": book},
            include=["documents", "metadatas"],
            batch_size=2000,
        ):
            label = build_reference_label(meta, doc)
            if normalize_reference(label) == normalize_reference(reference_label):
                return {"text": doc, "metadata": meta}
    except Exception as exc:
        print(f"âš ï¸ ëŒ€í‘œ êµ¬ì ˆ ì±… ê¸°ë°˜ ì¡°íšŒ ì‹¤íŒ¨ ({reference_label}): {exc}")
        return None
    return None


def get_or_create_curated_entry(normalized_key: str, reference_label: str):
    if not normalized_key:
        return None
    cached = REFERENCE_INDEX.get(normalized_key)
    if cached:
        return cached
    exact = get_exact_verse_entry(reference_label)
    if exact:
        REFERENCE_INDEX[normalized_key] = exact
        return exact
    fetched = lookup_reference_with_query(reference_label)
    if fetched:
        REFERENCE_INDEX[normalized_key] = fetched
        return fetched
    book_hit = lookup_reference_by_book(reference_label)
    if book_hit:
        REFERENCE_INDEX[normalized_key] = book_hit
        return book_hit
    return None


ensure_reference_index()
ensure_verse_lookup_index()

mailboxes = {}
postcards = {}


def build_contextual_query(keyword: str):
    """í‚¤ì›Œë“œë¥¼ ìƒí™© ì„¤ëª… ë¬¸ì¥ìœ¼ë¡œ í™•ì¥í•˜ê³ , í…Œë§ˆë³„ ëŒ€í‘œ êµ¬ì ˆ ëª©ë¡ë„ í•¨ê»˜ ë°˜í™˜."""
    keyword = (keyword or '').strip()
    lowered = keyword.lower()
    matched_contexts = []
    curated_refs = []

    for rule in THEME_CONTEXT_RULES:
        tokens = rule["tokens"]
        if any(token in keyword for token in tokens) or any(token in lowered for token in tokens):
            matched_contexts.append(rule["description"])
            curated_refs.extend(rule.get("curated_references", []))

    if not matched_contexts:
        matched_contexts.append(DEFAULT_CONTEXT_DESCRIPTION)

    # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìˆœì„œ ìœ ì§€
    seen_ctx = set()
    unique_contexts = []
    for ctx in matched_contexts:
        if ctx not in seen_ctx:
            unique_contexts.append(ctx)
            seen_ctx.add(ctx)

    seen_refs = set()
    unique_refs = []
    for ref in curated_refs:
        ref = ref.strip()
        if ref and ref not in seen_refs:
            unique_refs.append(ref)
            seen_refs.add(ref)

    contextual_summary = ' / '.join(unique_contexts)
    key_terms = greedy_terms(keyword)
    term_hint = f" í•µì‹¬ì–´: {', '.join(key_terms)}." if key_terms else ""
    expanded = (
        f"query: {keyword}. "
        f"ìƒí™©ê³¼ ê°ì •: {contextual_summary}.{term_hint} "
        "ì£¼ì œì™€ ë§ë‹¿ì€ ì„±ê²½ì˜ ì•½ì†, ìœ„ë¡œ, ê²©ë ¤, ë„ì „, í•˜ë‚˜ë‹˜ì˜ ì„±í’ˆê³¼ ê³„íšì„ ì°¾ëŠ”ë‹¤."
    )
    return expanded, unique_refs


@app.route('/')
def index():
    return render_template('index.html')


@app.route('/api/create-mailbox', methods=['POST'])
def create_mailbox():
    data = request.get_json(silent=True) or {}
    name = data.get('name')
    prayer_topic = data.get('prayer_topic', '')

    if not name:
        return jsonify({'error': 'Name is required'}), 400

    mailbox_id = str(uuid.uuid4())[:8]
    mailboxes[mailbox_id] = {
        'id': mailbox_id,
        'name': name,
        'nickname': name,
        'prayer_topic': prayer_topic,
        'url': f'/mailbox/{mailbox_id}',
        'created_at': datetime.now().isoformat(),
        'is_opened': False
    }
    postcards[mailbox_id] = []
    
    return jsonify({
        'mailbox_id': mailbox_id,
        'url': f'/mailbox/{mailbox_id}'
    })

@app.route('/api/recommend-verses', methods=['POST'])
def recommend_verses():
    """semantic ìš°ì„  + popularity + í…Œë§ˆ ëŒ€í‘œ êµ¬ì ˆ ì „ë¶€ ìƒë‹¨ ì£¼ì…"""
    if not bible_collection:
        return jsonify({'error': 'ChromaDB ì»¬ë ‰ì…˜ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}), 500
    
    try:
        data = request.json or {}
        query = (data.get('query') or data.get('keyword') or 'ì‚¬ë‘').strip()
        print(f"\nğŸ” ê²€ìƒ‰ ì…ë ¥: '{query}'")

        ensure_reference_index()

        # 0) ë ˆí¼ëŸ°ìŠ¤ ì§ì ‘ ì…ë ¥(ì˜ˆ: "ë§ˆ 10:5")ì´ë©´ í•´ë‹¹ êµ¬ì ˆì„ ìµœìš°ì„  ë°˜í™˜
        parsed_reference = parse_reference_input(query)
        exact_hit = get_exact_verse_entry(query)

        curated_reference_set = set()
        theme_injected = []
        phrase_query = query

        if exact_hit:
            meta = exact_hit.get("metadata") or {}
            doc = exact_hit.get("text", "")
            popularity = meta.get("popularity")
            if not isinstance(popularity, (int, float)):
                popularity = get_popularity_score(meta.get("source", ""), doc)
                meta["popularity"] = popularity
            if parsed_reference:
                reference = f"{parsed_reference['book']} {parsed_reference['chapter']}:{parsed_reference['verse']}"
            else:
                reference = meta.get("_reference_override") or build_reference_label(meta, doc)

            curated_reference_set.add(normalize_reference(reference))
            theme_injected.append({
                "text": doc,
                "reference": reference,
                "semantic_score": None,
                "popularity": popularity,
                "final_score": 2.0,
                "is_curated": False,
                "injected": True,
                "priority": "exact_reference",
            })
            query_text = f"query: {doc}. ì´ êµ¬ì ˆê³¼ ìœ ì‚¬í•œ ìœ„ë¡œ/ê²©ë ¤/ë„ì „ êµ¬ì ˆì„ ì°¾ëŠ”ë‹¤."
            phrase_query = None
            print(f"   ğŸ¯ ë ˆí¼ëŸ°ìŠ¤ ì§ì ‘ ë§¤ì¹­: {reference}")
        else:
            if parsed_reference:
                normalized_label = f"{parsed_reference['book']} {parsed_reference['chapter']}:{parsed_reference['verse']}"
                print(f"   âš ï¸ ë ˆí¼ëŸ°ìŠ¤ í˜•ì‹ ê°ì§€ë˜ì—ˆì§€ë§Œ ë¯¸ë°œê²¬: {normalized_label}")
            # 1) ì¿¼ë¦¬ë¥¼ ì£¼ì œ+ìƒí™©ìœ¼ë¡œ í™•ì¥
            query_text, curated_refs = build_contextual_query(query)

            # â­ 2) THEME ëŒ€í‘œ êµ¬ì ˆ ì „ë¶€ ë¨¼ì € í™•ë³´ (ì¤‘ë³µ ì œê±°)
            curated_keys_order = []
            for ref in curated_refs:
                key = normalize_reference(ref)
                if key and key not in curated_reference_set:
                    curated_reference_set.add(key)
                    curated_keys_order.append((key, ref))

            print(f"   ğŸ¯ ë§¤ì¹­ëœ í…Œë§ˆ ê·œì¹™: {len(curated_keys_order)}ê°œ ëŒ€í‘œ êµ¬ì ˆ")

            # ëŒ€í‘œ êµ¬ì ˆë“¤ì„ ë¨¼ì € ëª¨ë‘ í™•ë³´ (ìºì‹œ ë˜ëŠ” DBì—ì„œ)
            for key, original_label in curated_keys_order:
                cached = get_or_create_curated_entry(key, original_label)
                if not cached:
                    print(f"     âš ï¸ ëŒ€í‘œ êµ¬ì ˆ ë¯¸ë°œê²¬: {original_label}")
                    continue

                meta = cached.get("metadata") or {}
                doc = cached.get("text", "")
                popularity = meta.get("popularity", 85)
                reference = build_reference_label(meta, doc)

                theme_injected.append({
                    "text": doc,
                    "reference": reference,
                    "semantic_score": None,
                    "popularity": popularity,
                    "final_score": 1.8,  # í•­ìƒ ìµœìƒë‹¨ ê³ ì • ì ìˆ˜
                    "is_curated": True,
                    "injected": True,
                    "priority": "theme_top"
                })

            print(f"   âœ… í…Œë§ˆ ëŒ€í‘œ êµ¬ì ˆ {len(theme_injected)}ê°œ í™•ë³´ ì™„ë£Œ")

        # 3) ì¿¼ë¦¬ ì„ë² ë”© ë° ë²¡í„° ê²€ìƒ‰ (ëŒ€í‘œ êµ¬ì ˆ ì œì™¸í•˜ê³  ì¼ë°˜ ê²€ìƒ‰)
        query_embedding = embedding_model.encode(query_text).tolist()
        print(f"   ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(query_embedding)}ì°¨ì›")
        
        # êµ¬ì ˆ/ë¬¸êµ¬ ê²€ìƒ‰ì€ recallì´ ì¤‘ìš”í•˜ë¯€ë¡œ ì¡°ê¸ˆ ë” ë§ì´ ê°€ì ¸ì˜¨ ë’¤ rerank
        expanded_terms = greedy_terms(query) if not exact_hit else []
        n_results = 200 if (not exact_hit and (len(query) >= 6 or " " in query or expanded_terms)) else 40

        raw_results = bible_collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            include=["documents", "metadatas", "distances"]
        )
        print(f"âœ… 1ì°¨ ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(raw_results['documents'][0])}ê°œ ê²°ê³¼")
        
        # 4) ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ rerank (í…Œë§ˆ ëŒ€í‘œ êµ¬ì ˆ ì œì™¸)
        docs = raw_results["documents"][0]
        metas = raw_results["metadatas"][0]
        dists = raw_results["distances"][0]
        
        reranked_general = []
        
        for doc, meta, dist in zip(docs, metas, dists):
            reference = build_reference_label(meta, doc)
            normalized_ref = normalize_reference(reference)
            
            # ì´ë¯¸ í…Œë§ˆ ëŒ€í‘œ êµ¬ì ˆì´ë©´ ìŠ¤í‚µ
            if normalized_ref in curated_reference_set:
                continue
                
            semantic_score = 1 - dist
            popularity = meta.get("popularity")
            if not isinstance(popularity, (int, float)):
                popularity = get_popularity_score(meta.get("source", ""), doc)
                meta["popularity"] = popularity
            pop_norm = popularity / 100.0
            phrase_bonus = 0.15 if phrase_query and contains_phrase(phrase_query, doc) else 0.0
            greedy_hits = greedy_match_count(expanded_terms, doc) if expanded_terms else 0
            greedy_bonus = min(0.18, greedy_hits * 0.06)
            final_score = semantic_score * 0.6 + pop_norm * 0.4 + phrase_bonus + greedy_bonus
            
            reranked_general.append({
                "text": doc,
                "reference": reference,
                "semantic_score": round(semantic_score, 4),
                "popularity": popularity,
                "final_score": round(final_score, 4),
                "is_curated": False,
                "priority": "general"
            })
        
        # 5) ìµœì¢… ê²°ê³¼ ì¡°í•©: [í…Œë§ˆ ëŒ€í‘œ êµ¬ì ˆ ì „ë¶€] + [ì¼ë°˜ ìƒìœ„ ê²°ê³¼]
        reranked_general.sort(key=lambda x: x["final_score"], reverse=True)
        remaining_slots = max(0, 5 - len(theme_injected))
        final_results = theme_injected + reranked_general[:remaining_slots]
        if len(final_results) < 5:
            extra_needed = 5 - len(final_results)
            final_results.extend(reranked_general[remaining_slots:remaining_slots + extra_needed])
        top_k = final_results[:5]
        
        print("ğŸ“Œ ìµœì¢… ì„ íƒëœ êµ¬ì ˆ (í…Œë§ˆ ìš°ì„  + final_score):")
        for i, r in enumerate(top_k, 1):
            priority = r.get("priority", "general")
            print(f"  {i}. [{r['reference']}] {priority} | score={r['final_score']}")
            print(f"     {r['text'][:80]}...")
        
        return jsonify({"verses": top_k})
    
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"}), 500



def format_results(results):
    """ChromaDB ê²°ê³¼ë¥¼ í¬ë§·íŒ…í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    formatted = []
    if results['documents'] and results['documents'][0]:
        for i, doc in enumerate(results['documents'][0]):
            metadata = results['metadatas'][0][i] if results.get('metadatas') else {}
            distance = results['distances'][0][i] if results.get('distances') else 0
            
            reference = build_reference_label(metadata, doc)
            similarity_score = round((1 - distance) * 100, 1)
            popularity = metadata.get('popularity', 30)
            
            formatted.append({
                'text': doc,
                'reference': reference,
                'similarity': similarity_score,
                'popularity': popularity
            })
            
            print(f"  [{reference}] ìœ ì‚¬ë„: {similarity_score}% | ì¸ê¸°ë„: {popularity}")
    
    return formatted


@app.route('/api/send-postcard', methods=['POST'])
def send_postcard():
    data = request.json
    mailbox_id = data.get('mailbox_id')
    
    if mailbox_id not in mailboxes:
        return jsonify({'error': 'Mailbox not found'}), 404
    
    postcard = {
        'id': str(uuid.uuid4()),
        'verse_reference': data.get('verse_reference'),
        'verse_text': data.get('verse_text'),
        'message': data.get('message', ''),
        'created_at': datetime.now().isoformat()
    }
    
    postcards[mailbox_id].append(postcard)
    
    return jsonify({'success': True, 'postcard_id': postcard['id']})


@app.route('/mailbox/<mailbox_id>')
def mailbox(mailbox_id):
    if mailbox_id not in mailboxes:
        return "ìš°ì²´í†µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", 404
    
    mailbox_data = mailboxes[mailbox_id]
    
    if datetime.now() >= datetime(2026, 1, 1) or mailbox_data['is_opened']:
        mailbox_data['is_opened'] = True
        return render_template('mailbox.html', 
                             mailbox=mailbox_data, 
                             postcards=postcards.get(mailbox_id, []))
    else:
        return render_template('mailbox_locked.html', mailbox=mailbox_data)


@app.route('/send/<mailbox_id>')
def send_page(mailbox_id):
    if mailbox_id not in mailboxes:
        return "ìš°ì²´í†µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", 404
    
    return render_template('send_postcard.html', mailbox_id=mailbox_id)


def open_all_mailboxes():
    for mailbox_id in mailboxes:
        mailboxes[mailbox_id]['is_opened'] = True


scheduler = BackgroundScheduler()
scheduler.add_job(
    func=open_all_mailboxes,
    trigger='cron',
    year=2026,
    month=1,
    day=1,
    hour=0,
    minute=0
)
scheduler.start()


if __name__ == '__main__':
    print("\n" + "="*50)
    print("ğŸš€ Flask ì„œë²„ ì‹œì‘")
    print("âœ… ì‹œë§¨í‹± ê²€ìƒ‰ í™œì„±í™” (1024ì°¨ì› ë²¡í„°)")
    print("âœ… ì¸ê¸°ë„ í•„í„°ë§ í™œì„±í™” (3-tier ê²€ìƒ‰)")
    print("ğŸ“ ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†: http://127.0.0.1:5001")
    print("="*50 + "\n")
    app.run(host='127.0.0.1', port=5001, debug=True, threaded=True)
